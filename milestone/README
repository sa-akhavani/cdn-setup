We started by developing our DNS server, which would initially just return the IP address of the origin server. To
minimize the amount of code we needed to write and test, we opted not to write our own DNS response parser and instead
leveraged dnspython's internal types. The initial DNS server was trivial to write once we understood dnspython's types
and the full server was only around 50 lines of code. We tested our DNS server by sending dig requests to it and seeing
that the IP address of the origin server was returned in the response. We (Ali and Anthony) pair-programmed to write the
DNS server.

For our HTTP server, we used the Flask library, since we had read about how easy it was to use; and it was easy! Our
initial HTTP server, which simply received requests for /wiki/<name> and fetched the article from the origin server, was
written in under 20 lines of code. We tested our HTTP server by sending it requests via wget.

We then added a cache to our server. Our cache is simply a map that rejects additional put() operations once the total
size of all the values stored in it exceeds a given value (in this case, the  value is 10MB). Since the request
frequency for each piece of content follows a Zipf distribution, we decided not to implement any sort of cache
replacement, and rather plan to store the same most popular data at each replica statically.

In addition, our cache compresses all the data it stored using zlib. This will increase the amount of data we can store
at each replica server without going over the 10MB quota. Integrating our cache into the HTTP server was trivial. We
pair-programmed to write the HTTP server.

The last piece of data we implemented for this milestone was an interface for storing our entire cache in a file which
can be deployed to each replica server. Our file format is a list of JSON objects, with each object containing a
wikipedia article title and the zlib-compressed article body encoded in base64. The base64 encoding was necessary to
stop JSON from encoding the article in unicode, which completely mangled the data upon recovery from the file. Anthony
wrote this part of the code, and we tested it by requesting /wiki/Main_Page from the origin server, encoding it in the
JSON file, then reading it back from the file and ensuring that the retrieved message is the same as it was when it
arrived from the origin server.